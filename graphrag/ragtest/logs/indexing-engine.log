12:08:16,376 graphrag.cli.index INFO Logging enabled at C:\Users\Zhi Lin Ooi\Documents\GitHub\ZhilinR\APD.github.io\graphrag\ragtest\logs\indexing-engine.log
12:08:16,380 graphrag.cli.index INFO Starting pipeline run for: 20241210-120816, dry_run=False
12:08:16,381 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "encoding_model": "cl100k_base",
        "model": "gpt-4-turbo-preview",
        "embeddings_model": "text-embedding-3-small",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25,
        "responses": null
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Users\\Zhi Lin Ooi\\Documents\\GitHub\\ZhilinR\\APD.github.io\\graphrag\\ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\Zhi Lin Ooi\\Documents\\GitHub\\ZhilinR\\APD.github.io\\graphrag\\ragtest\\logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Users\\Zhi Lin Ooi\\Documents\\GitHub\\ZhilinR\\APD.github.io\\graphrag\\ragtest\\output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "encoding_model": "cl100k_base",
            "model": "text-embedding-3-small",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": {
            "type": "lancedb",
            "db_uri": "output\\lancedb",
            "container_name": "==== REDACTED ====",
            "overwrite": true
        },
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 3,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0.0,
        "local_search_top_p": 1.0,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 2000
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
12:08:16,409 graphrag.index.create_pipeline_config INFO skipping workflows 
12:08:16,410 graphrag.index.run.run INFO Running pipeline
12:08:16,410 graphrag.storage.file_pipeline_storage INFO Creating file storage at C:\Users\Zhi Lin Ooi\Documents\GitHub\ZhilinR\APD.github.io\graphrag\ragtest\output
12:08:16,416 graphrag.index.input.factory INFO loading input from root_dir=input
12:08:16,416 graphrag.index.input.factory INFO using file storage for input
12:08:16,418 graphrag.storage.file_pipeline_storage INFO search C:\Users\Zhi Lin Ooi\Documents\GitHub\ZhilinR\APD.github.io\graphrag\ragtest\input for files matching .*\.txt$
12:09:14,816 graphrag.cli.index INFO Logging enabled at C:\Users\Zhi Lin Ooi\Documents\GitHub\ZhilinR\APD.github.io\graphrag\ragtest\logs\indexing-engine.log
12:09:14,819 graphrag.cli.index INFO Starting pipeline run for: 20241210-120914, dry_run=False
12:09:14,820 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "encoding_model": "cl100k_base",
        "model": "gpt-4-turbo-preview",
        "embeddings_model": "text-embedding-3-small",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25,
        "responses": null
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Users\\Zhi Lin Ooi\\Documents\\GitHub\\ZhilinR\\APD.github.io\\graphrag\\ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\Zhi Lin Ooi\\Documents\\GitHub\\ZhilinR\\APD.github.io\\graphrag\\ragtest\\logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Users\\Zhi Lin Ooi\\Documents\\GitHub\\ZhilinR\\APD.github.io\\graphrag\\ragtest\\output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "encoding_model": "cl100k_base",
            "model": "text-embedding-3-small",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": {
            "type": "lancedb",
            "db_uri": "output\\lancedb",
            "container_name": "==== REDACTED ====",
            "overwrite": true
        },
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 3,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0.0,
        "local_search_top_p": 1.0,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 2000
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
12:09:14,822 graphrag.index.create_pipeline_config INFO skipping workflows 
12:09:14,823 graphrag.index.run.run INFO Running pipeline
12:09:14,823 graphrag.storage.file_pipeline_storage INFO Creating file storage at C:\Users\Zhi Lin Ooi\Documents\GitHub\ZhilinR\APD.github.io\graphrag\ragtest\output
12:09:14,824 graphrag.index.input.factory INFO loading input from root_dir=input
12:09:14,824 graphrag.index.input.factory INFO using file storage for input
12:09:14,826 graphrag.storage.file_pipeline_storage INFO search C:\Users\Zhi Lin Ooi\Documents\GitHub\ZhilinR\APD.github.io\graphrag\ragtest\input for files matching .*\.txt$
13:56:29,513 graphrag.cli.index INFO Logging enabled at C:\Users\Zhi Lin Ooi\Documents\GitHub\ZhilinR\APD.github.io\graphrag\ragtest\logs\indexing-engine.log
13:56:29,518 graphrag.cli.index INFO Starting pipeline run for: 20241210-135629, dry_run=False
13:56:29,518 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "encoding_model": "cl100k_base",
        "model": "gpt-4-turbo-preview",
        "embeddings_model": "text-embedding-3-small",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25,
        "responses": null
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Users\\Zhi Lin Ooi\\Documents\\GitHub\\ZhilinR\\APD.github.io\\graphrag\\ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\Zhi Lin Ooi\\Documents\\GitHub\\ZhilinR\\APD.github.io\\graphrag\\ragtest\\logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Users\\Zhi Lin Ooi\\Documents\\GitHub\\ZhilinR\\APD.github.io\\graphrag\\ragtest\\output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "encoding_model": "cl100k_base",
            "model": "text-embedding-3-small",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": {
            "type": "lancedb",
            "db_uri": "output\\lancedb",
            "container_name": "==== REDACTED ====",
            "overwrite": true
        },
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 3,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0.0,
        "local_search_top_p": 1.0,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 2000
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
13:56:29,521 graphrag.index.create_pipeline_config INFO skipping workflows 
13:56:29,522 graphrag.index.run.run INFO Running pipeline
13:56:29,522 graphrag.storage.file_pipeline_storage INFO Creating file storage at C:\Users\Zhi Lin Ooi\Documents\GitHub\ZhilinR\APD.github.io\graphrag\ragtest\output
13:56:29,522 graphrag.index.input.factory INFO loading input from root_dir=input
13:56:29,523 graphrag.index.input.factory INFO using file storage for input
13:56:29,524 graphrag.storage.file_pipeline_storage INFO search C:\Users\Zhi Lin Ooi\Documents\GitHub\ZhilinR\APD.github.io\graphrag\ragtest\input for files matching .*\.txt$
13:56:29,525 graphrag.index.input.text INFO found text files from input, found [('sample_code.txt', {})]
13:56:29,554 graphrag.index.input.text INFO Found 1 files, loading 1
13:56:29,556 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_final_documents', 'create_base_entity_graph', 'create_final_entities', 'create_final_relationships', 'create_final_nodes', 'create_final_communities', 'create_final_text_units', 'create_final_community_reports', 'generate_text_embeddings']
13:56:29,556 graphrag.index.run.run INFO Final # of rows loaded: 1
13:56:29,623 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
13:56:29,629 datashaper.workflow.workflow INFO executing verb create_base_text_units
13:56:38,190 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_text_units']
13:56:38,191 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:56:38,199 datashaper.workflow.workflow INFO executing verb create_final_documents
13:56:38,211 graphrag.index.exporter INFO exporting parquet table create_final_documents.parquet
13:56:38,331 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
13:56:38,332 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:56:38,342 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
13:56:45,809 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:56:49,50 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:57:13,757 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
13:57:13,758 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:57:13,770 datashaper.workflow.workflow INFO executing verb create_final_entities
13:57:13,773 graphrag.index.exporter INFO exporting parquet table create_final_entities.parquet
13:57:13,964 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph']
13:57:13,965 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:57:13,977 datashaper.workflow.workflow INFO executing verb create_final_relationships
13:57:13,986 graphrag.index.exporter INFO exporting parquet table create_final_relationships.parquet
13:57:14,158 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
13:57:14,158 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:57:14,174 datashaper.workflow.workflow INFO executing verb create_final_nodes
13:57:14,184 graphrag.index.exporter INFO exporting parquet table create_final_nodes.parquet
13:57:14,363 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
13:57:14,364 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:57:14,381 datashaper.workflow.workflow INFO executing verb create_final_communities
13:57:14,407 graphrag.index.exporter INFO exporting parquet table create_final_communities.parquet
13:57:14,577 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_entities', 'create_base_text_units', 'create_final_relationships']
13:57:14,577 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
13:57:14,597 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
13:57:14,598 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
13:57:14,637 datashaper.workflow.workflow INFO executing verb create_final_text_units
13:57:14,655 graphrag.index.exporter INFO exporting parquet table create_final_text_units.parquet
13:57:14,823 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_communities', 'create_final_nodes', 'create_final_entities', 'create_final_relationships']
13:57:14,823 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
13:57:14,848 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
13:57:14,861 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
13:57:14,866 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
13:57:14,893 datashaper.workflow.workflow INFO executing verb create_final_community_reports
13:57:14,908 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 4
13:57:31,513 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
13:57:31,533 graphrag.index.exporter INFO exporting parquet table create_final_community_reports.parquet
13:57:31,712 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_documents', 'create_final_entities', 'create_final_community_reports', 'create_final_text_units', 'create_final_relationships']
13:57:31,714 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
13:57:31,728 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
13:57:31,733 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
13:57:31,750 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
13:57:31,762 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
13:57:31,789 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
13:57:31,794 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
13:57:31,794 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
13:57:32,336 graphrag.index.operations.embed_text.strategies.openai INFO embedding 4 inputs via 4 snippets using 1 batches. max_batch_size=16, max_tokens=8191
13:57:36,127 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:57:36,652 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
13:57:37,216 graphrag.index.operations.embed_text.strategies.openai INFO embedding 1 inputs via 1 snippets using 1 batches. max_batch_size=16, max_tokens=8191
13:57:37,825 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:57:37,871 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
13:57:38,424 graphrag.index.operations.embed_text.strategies.openai INFO embedding 1 inputs via 1 snippets using 1 batches. max_batch_size=16, max_tokens=8191
13:57:39,429 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
13:57:39,574 graphrag.cli.index INFO All workflows completed successfully.
14:11:55,536 graphrag.cli.index INFO Logging enabled at C:\Users\Zhi Lin Ooi\Documents\GitHub\ZhilinR\APD.github.io\graphrag\ragtest\logs\indexing-engine.log
14:11:55,541 graphrag.cli.index INFO Starting pipeline run for: 20241210-141155, dry_run=False
14:11:55,541 graphrag.cli.index INFO Using default configuration: {
    "llm": {
        "api_key": "==== REDACTED ====",
        "type": "openai_chat",
        "encoding_model": "cl100k_base",
        "model": "gpt-4-turbo-preview",
        "embeddings_model": "text-embedding-3-small",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "frequency_penalty": 0.0,
        "presence_penalty": 0.0,
        "request_timeout": 180.0,
        "api_base": null,
        "api_version": null,
        "proxy": null,
        "audience": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25,
        "responses": null
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": "C:\\Users\\Zhi Lin Ooi\\Documents\\GitHub\\ZhilinR\\APD.github.io\\graphrag\\ragtest",
    "reporting": {
        "type": "file",
        "base_dir": "C:\\Users\\Zhi Lin Ooi\\Documents\\GitHub\\ZhilinR\\APD.github.io\\graphrag\\ragtest\\logs",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "C:\\Users\\Zhi Lin Ooi\\Documents\\GitHub\\ZhilinR\\APD.github.io\\graphrag\\ragtest\\output",
        "storage_account_blob_url": null
    },
    "update_index_storage": null,
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_embedding",
            "encoding_model": "cl100k_base",
            "model": "text-embedding-3-small",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": {
            "type": "lancedb",
            "db_uri": "output\\lancedb",
            "container_name": "==== REDACTED ====",
            "overwrite": true
        },
        "strategy": null
    },
    "chunks": {
        "size": 1200,
        "overlap": 100,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "embeddings": false,
        "graphml": false,
        "transient": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "==== REDACTED ====",
            "type": "openai_chat",
            "encoding_model": "cl100k_base",
            "model": "gpt-4-turbo-preview",
            "embeddings_model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "frequency_penalty": 0.0,
            "presence_penalty": 0.0,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "audience": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25,
            "responses": null
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "prompt": "prompts/local_search_system_prompt.txt",
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "map_prompt": "prompts/global_search_map_system_prompt.txt",
        "reduce_prompt": "prompts/global_search_reduce_system_prompt.txt",
        "knowledge_prompt": "prompts/global_search_knowledge_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32,
        "dynamic_search_llm": "gpt-4o-mini",
        "dynamic_search_threshold": 1,
        "dynamic_search_keep_parent": false,
        "dynamic_search_num_repeats": 1,
        "dynamic_search_use_summary": false,
        "dynamic_search_concurrent_coroutines": 16,
        "dynamic_search_max_level": 2
    },
    "drift_search": {
        "prompt": "prompts/drift_search_system_prompt.txt",
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 3,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "concurrency": 32,
        "drift_k_followups": 20,
        "primer_folds": 5,
        "primer_llm_max_tokens": 12000,
        "n_depth": 3,
        "local_search_text_unit_prop": 0.9,
        "local_search_community_prop": 0.1,
        "local_search_top_k_mapped_entities": 10,
        "local_search_top_k_relationships": 10,
        "local_search_max_data_tokens": 12000,
        "local_search_temperature": 0.0,
        "local_search_top_p": 1.0,
        "local_search_n": 1,
        "local_search_llm_max_gen_tokens": 2000
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
14:11:55,563 graphrag.index.create_pipeline_config INFO skipping workflows 
14:11:55,564 graphrag.index.run.run INFO Running pipeline
14:11:55,564 graphrag.storage.file_pipeline_storage INFO Creating file storage at C:\Users\Zhi Lin Ooi\Documents\GitHub\ZhilinR\APD.github.io\graphrag\ragtest\output
14:11:55,565 graphrag.index.input.factory INFO loading input from root_dir=input
14:11:55,565 graphrag.index.input.factory INFO using file storage for input
14:11:55,567 graphrag.storage.file_pipeline_storage INFO search C:\Users\Zhi Lin Ooi\Documents\GitHub\ZhilinR\APD.github.io\graphrag\ragtest\input for files matching .*\.txt$
14:11:55,571 graphrag.index.input.text INFO found text files from input, found [('src\\main\\java\\middlewareapd\\Main.txt', {}), ('src\\main\\java\\middlewareapd\\exception\\UnauthorizedException.txt', {}), ('src\\main\\java\\middlewareapd\\exception\\UserNotFoundException.txt', {}), ('src\\main\\java\\middlewareapd\\model\\JWToken.txt', {}), ('src\\main\\java\\middlewareapd\\repository\\MockJWTRepository.txt', {}), ('src\\main\\java\\middlewareapd\\service\\MiddlewareService.txt', {}), ('src\\main\\java\\middlewareapd\\util\\JwtUtil.txt', {}), ('src\\main\\java\\middlewareapd\\util\\LockFactory.txt', {}), ('src\\main\\java\\middlewareapd\\util\\ValidationUtil.txt', {}), ('src\\test\\java\\middlewareapd\\repository\\MockJWTRepositoryTest.txt', {}), ('src\\test\\java\\middlewareapd\\service\\MiddlewareConcurrencyTest.txt', {}), ('src\\test\\java\\middlewareapd\\service\\MiddlewareServiceTest.txt', {}), ('src\\test\\java\\middlewareapd\\util\\ValidationUtilTest.txt', {})]
14:11:55,728 graphrag.index.input.text INFO Found 13 files, loading 13
14:11:55,729 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_final_documents', 'create_base_entity_graph', 'create_final_entities', 'create_final_relationships', 'create_final_nodes', 'create_final_communities', 'create_final_text_units', 'create_final_community_reports', 'generate_text_embeddings']
14:11:55,730 graphrag.index.run.run INFO Final # of rows loaded: 13
14:11:55,791 graphrag.index.run.workflow INFO dependencies for create_base_text_units: []
14:11:55,798 datashaper.workflow.workflow INFO executing verb create_base_text_units
14:11:58,202 graphrag.index.run.workflow INFO dependencies for create_final_documents: ['create_base_text_units']
14:11:58,203 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:11:58,212 datashaper.workflow.workflow INFO executing verb create_final_documents
14:11:58,226 graphrag.index.exporter INFO exporting parquet table create_final_documents.parquet
14:11:58,373 graphrag.index.run.workflow INFO dependencies for create_base_entity_graph: ['create_base_text_units']
14:11:58,373 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:11:58,383 datashaper.workflow.workflow INFO executing verb create_base_entity_graph
14:12:05,943 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:05,946 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:05,948 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:05,950 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:05,952 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:05,954 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:05,957 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:05,963 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:05,966 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:06,349 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:07,351 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:07,493 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:07,503 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:07,566 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:07,732 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:07,760 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:07,761 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:07,913 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:08,101 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:08,105 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:09,952 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:10,88 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:10,125 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:10,190 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:10,431 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:10,527 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:10,542 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:11,40 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:11,322 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:11,825 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:12,85 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:12,151 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:12,936 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:13,594 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:13,841 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:14,381 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:14,482 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:14,600 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:14,753 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:15,301 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:15,378 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:15,433 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:15,684 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:15,753 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:15,857 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:15,876 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:15,962 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:16,177 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:18,266 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:18,474 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:19,352 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:21,391 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:21,926 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:23,389 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:23,664 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:24,129 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:24,254 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:24,406 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:24,508 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:24,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:24,725 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:24,817 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:25,253 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:30,610 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:33,639 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:34,405 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:34,593 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:34,663 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:34,852 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:34,866 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:35,9 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:35,89 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:35,548 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:36,459 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:38,288 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:38,951 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:41,740 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:44,788 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:44,945 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:45,329 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:45,393 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:45,882 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:46,906 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:47,150 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:48,530 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:51,746 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:12:55,194 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:55,612 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:55,674 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:56,156 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:12:59,520 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:00,324 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:05,896 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:13:05,944 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:13:06,443 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:13:07,693 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:08,564 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:16,204 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:13:16,275 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'prompt': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: package middlewareapd;\n\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.TimeUnit;\n\nimport middlewareapd.model.JWToken;\nimport middlewareapd.service.MiddlewareService;\nimport middlewareapd.repository.MockJWTRepository;\nimport middlewareapd.exception.UnauthorizedException;\n\n/**\n * The {@code Main} class serves as the entry point for the application,\n * which validates JWT tokens using a specified {@link MiddlewareService}.\n * It demonstrates both threaded and sequential validation approaches, \n * comparing their execution times.\n */\npublic class Main {\n\n    /**\n     * The main method initializes the application, sets up the mock JWT repository,\n     * and runs validation tests both with threads and sequentially.\n     *\n     * @param args command line arguments (not used)\n     */\n    public static void main(String[] args) {\n        // Initialize MockJWTRepository\n        MockJWTRepository mockRepository = new MockJWTRepository();\n\n        // Initialize MiddlewareService with the mock repository\n        MiddlewareService middlewareService = new MiddlewareService(mockRepository);\n\n        // Get all mock tokens from the repository\n        List<JWToken> mockTokens = mockRepository.getAllTokens();\n\n        // Run validations with threads and time it\n        long threadedTime = runValidationWithThreads(mockTokens, middlewareService);\n\n        // Run validations sequentially and time it\n        long sequentialTime = runValidationSequentially(mockTokens, middlewareService);\n\n        System.out.println("Time taken for threaded validation: " + threadedTime + " ms");\n        System.out.println("Time taken for sequential validation: " + sequentialTime + " ms");\n    }\n\n    /**\n     * Runs validation on a list of JWT tokens using multiple threads.\n     *\n     * @param tokens the list of JWT tokens to validate.\n     * @param middlewareService the {@code MiddlewareService} used for validation.\n     * @return the time taken to complete the validation, in milliseconds.\n     */\n    private static long runValidationWithThreads(List<JWToken> tokens, MiddlewareService middlewareService) {\n        long startTime = System.currentTimeMillis();\n\n        // Create a fixed thread pool to process tokens concurrently\n        ExecutorService executorService = Executors.newFixedThreadPool(50); // Adjust as needed\n\n        System.out.println("Validating Mock Tokens with Threads:");\n\n        // Submit a validation task for each token\n        for (JWToken token : tokens) {\n            executorService.submit(() -> {\n                try {\n                    validateToken(token, middlewareService);\n                } catch (Exception e) {\n                    System.err.println("Error validating token: " + token.getUuid() + " - " + e.getMessage());\n                    e.printStackTrace();\n                }\n            });\n        }\n\n        // Shutdown the executor service gracefully\n        shutdownExecutorService(executorService);\n\n        return System.currentTimeMillis() - startTime;\n    }\n\n    /**\n     * Validates tokens sequentially using the specified {@code MiddlewareService} \n     * and returns the time taken.\n     *\n     * @param tokens the list of JWT tokens to validate.\n     * @param middlewareService the service used for JWT validation.\n     * @return the time taken for the validation in milliseconds.\n     */\n    private static long runValidationSequentially(List<JWToken> tokens, MiddlewareService middlewareService) {\n        long startTime = System.currentTimeMillis();\n\n        System.out.println("Validating Mock Tokens Sequentially:");\n\n        for (JWToken token : tokens) {\n            validateToken(token, middlewareService);\n        }\n\n        return System.currentTimeMillis() - startTime;\n    }\n\n    /**\n     * Validates a single token and prints the result.\n     *\n     * @param token the {@code JWToken} to validate.\n     * @param middlewareService the service used for JWT validation.\n     */\n    private static void validateToken(JWToken token, MiddlewareService middlewareService) {\n        try {\n            // Attempt to validate the token\'s JWT\n            Map<String, Object> result = middlewareService.checkJwt(token.getJwt());\n\n            // If successful, print the result\n            System.out.println("Token valid. UUID: " + result.get("uuid") + ", isAdmin: " + result.get("isAdmin"));\n        } catch (UnauthorizedException e) {\n            System.out.println("Invalid token: " + token.getJwt() + ". Reason: " + e.getMessage());\n        }\n    }\n\n    /**\n     * Shuts down the executor service gracefully, waiting for tasks to complete.\n     *\n     * @param executorService the executor service to shut down.\n     */\n    private static void shutdownExecutorService(ExecutorService executorService) {\n        executorService.shutdown(); // Initiate shutdown\n\n        try {\n            if (!executorService.awaitTermination(300, TimeUnit.SECONDS)) {\n                System.err.println("Executor did not terminate in the specified time. Forcing shutdown...");\n\n                // Force shutdown and log unfinished tasks\n                List<Runnable> unfinishedTasks = executorService.shutdownNow();\n                System.err.println("Unfinished tasks: " + unfinishedTasks.size());\n            }\n        } catch (InterruptedException e) {\n            System.err.println("Shutdown interrupted. Forcing shutdown...");\n            executorService.shutdownNow(); // Force shutdown if interrupted\n            Thread.currentThread().interrupt(); // Preserve interrupt status\n        }\n    }\n}\n######################\nOutput:', 'kwargs': {}}
14:13:16,276 graphrag.index.graph.extractors.graph.graph_extractor ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 127, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 155, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\openai\llm\chat.py", line 83, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\openai\llm\features\tools_parsing.py", line 120, in __call__
    return await self._delegate(prompt, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\base\base.py", line 112, in __call__
    return await self._invoke(prompt, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\base\base.py", line 128, in _invoke
    return await self._decorated_target(prompt, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\services\json.py", line 71, in invoke
    return await delegate(prompt, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\services\retryer.py", line 109, in invoke
    result = await execute_with_retry()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\services\retryer.py", line 106, in execute_with_retry
    raise RetriesExhaustedError(name, self._max_retries)
fnllm.services.errors.RetriesExhaustedError: Operation 'chat' failed - 10 retries exhausted.
14:13:16,325 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'package middlewareapd;\n\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.TimeUnit;\n\nimport middlewareapd.model.JWToken;\nimport middlewareapd.service.MiddlewareService;\nimport middlewareapd.repository.MockJWTRepository;\nimport middlewareapd.exception.UnauthorizedException;\n\n/**\n * The {@code Main} class serves as the entry point for the application,\n * which validates JWT tokens using a specified {@link MiddlewareService}.\n * It demonstrates both threaded and sequential validation approaches, \n * comparing their execution times.\n */\npublic class Main {\n\n    /**\n     * The main method initializes the application, sets up the mock JWT repository,\n     * and runs validation tests both with threads and sequentially.\n     *\n     * @param args command line arguments (not used)\n     */\n    public static void main(String[] args) {\n        // Initialize MockJWTRepository\n        MockJWTRepository mockRepository = new MockJWTRepository();\n\n        // Initialize MiddlewareService with the mock repository\n        MiddlewareService middlewareService = new MiddlewareService(mockRepository);\n\n        // Get all mock tokens from the repository\n        List<JWToken> mockTokens = mockRepository.getAllTokens();\n\n        // Run validations with threads and time it\n        long threadedTime = runValidationWithThreads(mockTokens, middlewareService);\n\n        // Run validations sequentially and time it\n        long sequentialTime = runValidationSequentially(mockTokens, middlewareService);\n\n        System.out.println("Time taken for threaded validation: " + threadedTime + " ms");\n        System.out.println("Time taken for sequential validation: " + sequentialTime + " ms");\n    }\n\n    /**\n     * Runs validation on a list of JWT tokens using multiple threads.\n     *\n     * @param tokens the list of JWT tokens to validate.\n     * @param middlewareService the {@code MiddlewareService} used for validation.\n     * @return the time taken to complete the validation, in milliseconds.\n     */\n    private static long runValidationWithThreads(List<JWToken> tokens, MiddlewareService middlewareService) {\n        long startTime = System.currentTimeMillis();\n\n        // Create a fixed thread pool to process tokens concurrently\n        ExecutorService executorService = Executors.newFixedThreadPool(50); // Adjust as needed\n\n        System.out.println("Validating Mock Tokens with Threads:");\n\n        // Submit a validation task for each token\n        for (JWToken token : tokens) {\n            executorService.submit(() -> {\n                try {\n                    validateToken(token, middlewareService);\n                } catch (Exception e) {\n                    System.err.println("Error validating token: " + token.getUuid() + " - " + e.getMessage());\n                    e.printStackTrace();\n                }\n            });\n        }\n\n        // Shutdown the executor service gracefully\n        shutdownExecutorService(executorService);\n\n        return System.currentTimeMillis() - startTime;\n    }\n\n    /**\n     * Validates tokens sequentially using the specified {@code MiddlewareService} \n     * and returns the time taken.\n     *\n     * @param tokens the list of JWT tokens to validate.\n     * @param middlewareService the service used for JWT validation.\n     * @return the time taken for the validation in milliseconds.\n     */\n    private static long runValidationSequentially(List<JWToken> tokens, MiddlewareService middlewareService) {\n        long startTime = System.currentTimeMillis();\n\n        System.out.println("Validating Mock Tokens Sequentially:");\n\n        for (JWToken token : tokens) {\n            validateToken(token, middlewareService);\n        }\n\n        return System.currentTimeMillis() - startTime;\n    }\n\n    /**\n     * Validates a single token and prints the result.\n     *\n     * @param token the {@code JWToken} to validate.\n     * @param middlewareService the service used for JWT validation.\n     */\n    private static void validateToken(JWToken token, MiddlewareService middlewareService) {\n        try {\n            // Attempt to validate the token\'s JWT\n            Map<String, Object> result = middlewareService.checkJwt(token.getJwt());\n\n            // If successful, print the result\n            System.out.println("Token valid. UUID: " + result.get("uuid") + ", isAdmin: " + result.get("isAdmin"));\n        } catch (UnauthorizedException e) {\n            System.out.println("Invalid token: " + token.getJwt() + ". Reason: " + e.getMessage());\n        }\n    }\n\n    /**\n     * Shuts down the executor service gracefully, waiting for tasks to complete.\n     *\n     * @param executorService the executor service to shut down.\n     */\n    private static void shutdownExecutorService(ExecutorService executorService) {\n        executorService.shutdown(); // Initiate shutdown\n\n        try {\n            if (!executorService.awaitTermination(300, TimeUnit.SECONDS)) {\n                System.err.println("Executor did not terminate in the specified time. Forcing shutdown...");\n\n                // Force shutdown and log unfinished tasks\n                List<Runnable> unfinishedTasks = executorService.shutdownNow();\n                System.err.println("Unfinished tasks: " + unfinishedTasks.size());\n            }\n        } catch (InterruptedException e) {\n            System.err.println("Shutdown interrupted. Forcing shutdown...");\n            executorService.shutdownNow(); // Force shutdown if interrupted\n            Thread.currentThread().interrupt(); // Preserve interrupt status\n        }\n    }\n}'}
14:13:16,756 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
14:13:16,758 graphrag.callbacks.file_workflow_callbacks INFO Error Invoking LLM details={'prompt': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: package middlewareapd.service;\n\nimport static org.junit.jupiter.api.Assertions.*;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\n\nimport java.lang.reflect.Method;\nimport java.time.LocalDateTime;\nimport java.util.concurrent.locks.ReadWriteLock;\n\nimport middlewareapd.repository.*;\nimport middlewareapd.model.*;\nimport middlewareapd.util.*;\nimport middlewareapd.exception.*;\n\n/**\n * Unit tests for the {@link MiddlewareService} class.\n * <p>\n * This test class ensures that the concurrency handling, race conditions, and lock management\n * within the {@link MiddlewareService} works as expected. The {@link MockJWTRepository} is mocked \n * to simulate the behavior of the repository without interacting with the actual data source.\n * </p>\n *\n * <b>Test Methods:</b>\n * <ul>\n *     <li>{@link #testConcurrentTokenUpdate()} - Validates that concurrent updates to the same token \n *     are handled properly by using a write lock to prevent race conditions.</li>\n *     <li>{@link #testConcurrentLogoutDuringValidation()} - Ensures that a race condition between \n *     JWT validation and user logout is correctly managed, with logout taking precedence.</li>\n *     <li>{@link #testReadWriteLockBehavior()} - Validates the behavior of read-write locks during \n *     concurrent operations, ensuring multiple reads are allowed but writes block both reads and other writes.</li>\n * </ul>\n */\npublic class MiddlewareConcurrencyTest {\n\n    private MockJWTRepository mockRepo;\n    private MiddlewareService service;\n    private String uuid;\n    private JWToken token;\n\n    /**\n     * Setup method to initialize required objects before each test case.\n     */\n    @BeforeEach\n    public void setup() {\n        mockRepo = new MockJWTRepository();\n        service = new MiddlewareService(mockRepo);\n        uuid = "test-uuid";\n\n        // Generate a valid JWT using JwtUtil\n        String jwt = JwtUtil.generateToken("user@example.com", uuid, 1);\n\n        // Initialize a token with all required fields\n        token = new JWToken(jwt, uuid, 1, LocalDateTime.now(), null, LocalDateTime.now());\n        mockRepo.addToken(token); // Add the token to the mock repository\n    }\n\n    /**\n     * Test to validate that concurrent updates to the same token are handled properly\n     * by using a write lock to prevent race conditions.\n     *\n     * @throws InterruptedException if thread execution is interrupted\n     */\n    @Test\n    public void testConcurrentTokenUpdate() throws Exception {\n        // Arrange: Add a token to the repository\n        JWToken token = new JWToken("test-jwt", uuid, 0, \n                                    LocalDateTime.now(), null, LocalDateTime.now());\n        mockRepo.addToken(token);\n\n        // Use Reflection to access the private \'updateLastAccess\' method\n        Method updateMethod = MiddlewareService.class.getDeclaredMethod(\n                "updateLastAccess", String.class, ReadWriteLock.class);\n        updateMethod.setAccessible(true); // Make the private method accessible\n\n        // Define a task that calls \'updateLastAccess\' using reflection\n        Runnable task = () -> {\n            try {\n                ReadWriteLock lock = LockFactory.getRWLock(token.getJwt());\n                updateMethod.invoke(service, uuid, lock); // Invoke using reflection\n            } catch (Exception e) {\n                throw new RuntimeException(e); // Handle any reflection exceptions\n            }\n        };\n\n        // Define and start two threads simulating concurrent updates\n        Thread thread1 = new Thread(task);\n        Thread thread2 = new Thread(task);\n\n        thread1.start();\n        thread2.start();\n\n        // Wait for both threads to complete\n        thread1.join();\n        thread2.join();\n\n        // Assert that the token\'s last access was updated\n        assertNotNull(mockRepo.getTokenByUuid(uuid).getLastAccess());\n        System.out.println("Concurrent update handled successfully.");\n    }\n\n    /**\n     * Test to validate that a race condition between JWT validation and user logout\n     * is correctly managed by ensuring that the logout action takes precedence.\n     *\n     * @throws InterruptedException if thread execution is interrupted\n     */\n    @Test\n    public void testConcurrentLogoutDuringValidation() throws InterruptedException {\n        // Generate a valid JWT for the test\n        String jwt = JwtUtil.generateToken("user@example.com", uuid, 1);\n\n        // Thread 1: Validate the JWT token\n        Runnable validateTask = () -> {\n            try {\n                service.checkJwt(jwt);\n            } catch (UnauthorizedException e) {\n                System.out.println("JWT validation failed: " + e.getMessage());\n            }\n        };\n\n        // Thread 2: Simulate user logging out\n        Runnable logoutTask = () -> {\n            token.setLogout(LocalDateTime.now());\n            mockRepo.updateToken(token);\n        };\n\n        // Start both tasks concurrently\n        Thread thread1 = new Thread(validateTask);\n        Thread thread2 = new Thread(logoutTask);\n\n        thread1.start();\n        thread2.start();\n\n        // Wait for both threads to finish\n        thread1.join();\n        thread2.join();\n\n        // Verify that the token\'s logout timestamp was set\n        assertNotNull(mockRepo.getTokenByUuid(uuid).getLogout());\n        System.out.println("Race condition handled: Logout takes precedence.");\n    }\n\n    /**\n     * Test to validate the behavior of read-write locks during concurrent operations.\n     * Ensures that multiple read operations are allowed concurrently but write operations\n     * block both reads and other writes.\n     *\n     * @throws InterruptedException if thread execution is interrupted\n     */\n    @Test\n    public void testReadWriteLockBehavior() throws InterruptedException {\n        ReadWriteLock lock = LockFactory.getRWLock(uuid);\n\n        // Task: Simulate a read operation\n        Runnable readTask = () -> {\n            lock.readLock().lock();\n            try {\n                System.out.println("Reading token for UUID: " + uuid);\n            }\n######################\nOutput:', 'kwargs': {}}
14:13:16,759 graphrag.index.graph.extractors.graph.graph_extractor ERROR error extracting graph
Traceback (most recent call last):
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 127, in __call__
    result = await self._process_document(text, prompt_variables)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\graphrag\index\graph\extractors\graph\graph_extractor.py", line 155, in _process_document
    response = await self._llm(
               ^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\openai\llm\chat.py", line 83, in __call__
    return await self._text_chat_llm(prompt, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\openai\llm\features\tools_parsing.py", line 120, in __call__
    return await self._delegate(prompt, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\base\base.py", line 112, in __call__
    return await self._invoke(prompt, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\base\base.py", line 128, in _invoke
    return await self._decorated_target(prompt, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\services\json.py", line 71, in invoke
    return await delegate(prompt, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\services\retryer.py", line 109, in invoke
    result = await execute_with_retry()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Zhi Lin Ooi\AppData\Local\Programs\Python\Python312\Lib\site-packages\fnllm\services\retryer.py", line 106, in execute_with_retry
    raise RetriesExhaustedError(name, self._max_retries)
fnllm.services.errors.RetriesExhaustedError: Operation 'chat' failed - 10 retries exhausted.
14:13:16,761 graphrag.callbacks.file_workflow_callbacks INFO Entity Extraction Error details={'doc_index': 0, 'text': 'package middlewareapd.service;\n\nimport static org.junit.jupiter.api.Assertions.*;\nimport org.junit.jupiter.api.BeforeEach;\nimport org.junit.jupiter.api.Test;\n\nimport java.lang.reflect.Method;\nimport java.time.LocalDateTime;\nimport java.util.concurrent.locks.ReadWriteLock;\n\nimport middlewareapd.repository.*;\nimport middlewareapd.model.*;\nimport middlewareapd.util.*;\nimport middlewareapd.exception.*;\n\n/**\n * Unit tests for the {@link MiddlewareService} class.\n * <p>\n * This test class ensures that the concurrency handling, race conditions, and lock management\n * within the {@link MiddlewareService} works as expected. The {@link MockJWTRepository} is mocked \n * to simulate the behavior of the repository without interacting with the actual data source.\n * </p>\n *\n * <b>Test Methods:</b>\n * <ul>\n *     <li>{@link #testConcurrentTokenUpdate()} - Validates that concurrent updates to the same token \n *     are handled properly by using a write lock to prevent race conditions.</li>\n *     <li>{@link #testConcurrentLogoutDuringValidation()} - Ensures that a race condition between \n *     JWT validation and user logout is correctly managed, with logout taking precedence.</li>\n *     <li>{@link #testReadWriteLockBehavior()} - Validates the behavior of read-write locks during \n *     concurrent operations, ensuring multiple reads are allowed but writes block both reads and other writes.</li>\n * </ul>\n */\npublic class MiddlewareConcurrencyTest {\n\n    private MockJWTRepository mockRepo;\n    private MiddlewareService service;\n    private String uuid;\n    private JWToken token;\n\n    /**\n     * Setup method to initialize required objects before each test case.\n     */\n    @BeforeEach\n    public void setup() {\n        mockRepo = new MockJWTRepository();\n        service = new MiddlewareService(mockRepo);\n        uuid = "test-uuid";\n\n        // Generate a valid JWT using JwtUtil\n        String jwt = JwtUtil.generateToken("user@example.com", uuid, 1);\n\n        // Initialize a token with all required fields\n        token = new JWToken(jwt, uuid, 1, LocalDateTime.now(), null, LocalDateTime.now());\n        mockRepo.addToken(token); // Add the token to the mock repository\n    }\n\n    /**\n     * Test to validate that concurrent updates to the same token are handled properly\n     * by using a write lock to prevent race conditions.\n     *\n     * @throws InterruptedException if thread execution is interrupted\n     */\n    @Test\n    public void testConcurrentTokenUpdate() throws Exception {\n        // Arrange: Add a token to the repository\n        JWToken token = new JWToken("test-jwt", uuid, 0, \n                                    LocalDateTime.now(), null, LocalDateTime.now());\n        mockRepo.addToken(token);\n\n        // Use Reflection to access the private \'updateLastAccess\' method\n        Method updateMethod = MiddlewareService.class.getDeclaredMethod(\n                "updateLastAccess", String.class, ReadWriteLock.class);\n        updateMethod.setAccessible(true); // Make the private method accessible\n\n        // Define a task that calls \'updateLastAccess\' using reflection\n        Runnable task = () -> {\n            try {\n                ReadWriteLock lock = LockFactory.getRWLock(token.getJwt());\n                updateMethod.invoke(service, uuid, lock); // Invoke using reflection\n            } catch (Exception e) {\n                throw new RuntimeException(e); // Handle any reflection exceptions\n            }\n        };\n\n        // Define and start two threads simulating concurrent updates\n        Thread thread1 = new Thread(task);\n        Thread thread2 = new Thread(task);\n\n        thread1.start();\n        thread2.start();\n\n        // Wait for both threads to complete\n        thread1.join();\n        thread2.join();\n\n        // Assert that the token\'s last access was updated\n        assertNotNull(mockRepo.getTokenByUuid(uuid).getLastAccess());\n        System.out.println("Concurrent update handled successfully.");\n    }\n\n    /**\n     * Test to validate that a race condition between JWT validation and user logout\n     * is correctly managed by ensuring that the logout action takes precedence.\n     *\n     * @throws InterruptedException if thread execution is interrupted\n     */\n    @Test\n    public void testConcurrentLogoutDuringValidation() throws InterruptedException {\n        // Generate a valid JWT for the test\n        String jwt = JwtUtil.generateToken("user@example.com", uuid, 1);\n\n        // Thread 1: Validate the JWT token\n        Runnable validateTask = () -> {\n            try {\n                service.checkJwt(jwt);\n            } catch (UnauthorizedException e) {\n                System.out.println("JWT validation failed: " + e.getMessage());\n            }\n        };\n\n        // Thread 2: Simulate user logging out\n        Runnable logoutTask = () -> {\n            token.setLogout(LocalDateTime.now());\n            mockRepo.updateToken(token);\n        };\n\n        // Start both tasks concurrently\n        Thread thread1 = new Thread(validateTask);\n        Thread thread2 = new Thread(logoutTask);\n\n        thread1.start();\n        thread2.start();\n\n        // Wait for both threads to finish\n        thread1.join();\n        thread2.join();\n\n        // Verify that the token\'s logout timestamp was set\n        assertNotNull(mockRepo.getTokenByUuid(uuid).getLogout());\n        System.out.println("Race condition handled: Logout takes precedence.");\n    }\n\n    /**\n     * Test to validate the behavior of read-write locks during concurrent operations.\n     * Ensures that multiple read operations are allowed concurrently but write operations\n     * block both reads and other writes.\n     *\n     * @throws InterruptedException if thread execution is interrupted\n     */\n    @Test\n    public void testReadWriteLockBehavior() throws InterruptedException {\n        ReadWriteLock lock = LockFactory.getRWLock(uuid);\n\n        // Task: Simulate a read operation\n        Runnable readTask = () -> {\n            lock.readLock().lock();\n            try {\n                System.out.println("Reading token for UUID: " + uuid);\n            }'}
14:13:25,673 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:26,718 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:41,410 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:41,484 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:41,678 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:42,103 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:43,460 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:43,677 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:44,90 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:44,707 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:44,752 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:13:44,988 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:14:04,194 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:14:04,936 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:14:15,184 graphrag.index.run.workflow INFO dependencies for create_final_entities: ['create_base_entity_graph']
14:14:15,185 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:14:15,196 datashaper.workflow.workflow INFO executing verb create_final_entities
14:14:15,198 graphrag.index.exporter INFO exporting parquet table create_final_entities.parquet
14:14:15,352 graphrag.index.run.workflow INFO dependencies for create_final_relationships: ['create_base_entity_graph']
14:14:15,353 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:14:15,365 datashaper.workflow.workflow INFO executing verb create_final_relationships
14:14:15,373 graphrag.index.exporter INFO exporting parquet table create_final_relationships.parquet
14:14:15,552 graphrag.index.run.workflow INFO dependencies for create_final_nodes: ['create_base_entity_graph']
14:14:15,552 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:14:15,566 datashaper.workflow.workflow INFO executing verb create_final_nodes
14:14:15,574 graphrag.index.exporter INFO exporting parquet table create_final_nodes.parquet
14:14:15,719 graphrag.index.run.workflow INFO dependencies for create_final_communities: ['create_base_entity_graph']
14:14:15,720 graphrag.index.run.workflow WARNING Dependency table create_base_entity_graph not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:14:15,735 datashaper.workflow.workflow INFO executing verb create_final_communities
14:14:15,755 graphrag.index.exporter INFO exporting parquet table create_final_communities.parquet
14:14:15,896 graphrag.index.run.workflow INFO dependencies for create_final_text_units: ['create_final_relationships', 'create_final_entities', 'create_base_text_units']
14:14:15,897 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
14:14:15,917 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
14:14:15,932 graphrag.index.run.workflow WARNING Dependency table create_base_text_units not found in storage: it may be a runtime-only in-memory table. If you see further errors, this may be an actual problem.
14:14:15,952 datashaper.workflow.workflow INFO executing verb create_final_text_units
14:14:15,979 graphrag.index.exporter INFO exporting parquet table create_final_text_units.parquet
14:14:16,187 graphrag.index.run.workflow INFO dependencies for create_final_community_reports: ['create_final_relationships', 'create_final_entities', 'create_final_communities', 'create_final_nodes']
14:14:16,187 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
14:14:16,193 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
14:14:16,198 graphrag.utils.storage INFO read table from storage: create_final_communities.parquet
14:14:16,212 graphrag.utils.storage INFO read table from storage: create_final_nodes.parquet
14:14:16,242 datashaper.workflow.workflow INFO executing verb create_final_community_reports
14:14:16,254 graphrag.index.operations.summarize_communities.prepare_community_reports INFO Number of nodes at level=0 => 19
14:14:31,416 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:14:35,763 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:14:36,945 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:14:39,447 httpx INFO HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 200 OK"
14:14:39,465 graphrag.index.exporter INFO exporting parquet table create_final_community_reports.parquet
14:14:39,622 graphrag.index.run.workflow INFO dependencies for generate_text_embeddings: ['create_final_documents', 'create_final_relationships', 'create_final_entities', 'create_final_text_units', 'create_final_community_reports']
14:14:39,628 graphrag.utils.storage INFO read table from storage: create_final_documents.parquet
14:14:39,659 graphrag.utils.storage INFO read table from storage: create_final_relationships.parquet
14:14:39,665 graphrag.utils.storage INFO read table from storage: create_final_entities.parquet
14:14:39,670 graphrag.utils.storage INFO read table from storage: create_final_text_units.parquet
14:14:39,688 graphrag.utils.storage INFO read table from storage: create_final_community_reports.parquet
14:14:39,718 datashaper.workflow.workflow INFO executing verb generate_text_embeddings
14:14:39,721 graphrag.index.flows.generate_text_embeddings INFO Creating embeddings
14:14:39,722 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding entity.description: default-entity-description
14:14:40,418 graphrag.index.operations.embed_text.strategies.openai INFO embedding 19 inputs via 19 snippets using 2 batches. max_batch_size=16, max_tokens=8191
14:14:41,80 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:14:41,271 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:14:42,110 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding community.full_content: default-community-full_content
14:14:42,567 graphrag.index.operations.embed_text.strategies.openai INFO embedding 4 inputs via 4 snippets using 1 batches. max_batch_size=16, max_tokens=8191
14:14:43,159 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:14:43,438 graphrag.index.operations.embed_text.embed_text INFO using vector store lancedb with container_name default for embedding text_unit.text: default-text_unit-text
14:14:43,908 graphrag.index.operations.embed_text.strategies.openai INFO embedding 15 inputs via 15 snippets using 2 batches. max_batch_size=16, max_tokens=8191
14:14:44,508 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:14:44,812 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
14:14:45,524 graphrag.cli.index INFO All workflows completed successfully.
